<h1>HuggingFace Models: https://huggingface.co/models </h1>

<h4>Long-Llama_250k</h4>
1. https://huggingface.co/syzymon/long_llama_3b; LongLLaMA is an OpenLLaMA model finetuned with the FoT method, with three layers used for context extension. Crucially, LongLLama is able to extrapolate much beyond the context length seen in training: 8k. E.g., in the key retrieval task, it can handle inputs of length 256k.
<h4>next_model</h4>
2. etc. 