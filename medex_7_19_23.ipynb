{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP7tKv4KItxx/A6wvKb8hhO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/acdc-digital/Medex-Public-MITP/blob/main/medex_7_19_23.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Welcome to the Latest:"
      ],
      "metadata": {
        "id": "bkyitkCPFHRf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Welcome! Below you'll find the latest Medex ingest files. Our goal is to ensure we're capturing ALL of the user information on their medical chart. While there are still some complexitites being worked-on in the background, the initial functionality will act as a high-performance doc-search, and then we'll begin to implement our healthcare specific attributes. The below is the current development for the Medex ingestion, including custom loaders/ source-file location and variables/ splitting/ chunking/ embedding/ and vectorstore. The cherry on top? We've re-initiated the Llamma-Index to improve the robustness of our application. I am very excited to share our progress to this point!"
      ],
      "metadata": {
        "id": "dFV9PL7gELiI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain\n",
        "!pip install llama-index\n",
        "!pip install cohers\n",
        "!pip install milvus\n",
        "!pip install pymilvus\n",
        "!pip install python-dotenv\n",
        "!pip install nltk\n",
        "!pip install numpy\n",
        "!pip install tdqm\n",
        "!pip install pdfminer.six\n",
        "!pip install pyPDF2\n",
        "!pip install tesseract"
      ],
      "metadata": {
        "id": "DpTF94MBDvWW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Show dependy and version information in case of error."
      ],
      "metadata": {
        "id": "OPwJ9uDJD5aV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip show langchain"
      ],
      "metadata": {
        "id": "2avFzeH6DyUD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ingest.py / under construction as of 7/19/2023. We'll be finalizing this file shortly, and then we'll implement the retriever function. More information on those components/modules will be coming in the following days."
      ],
      "metadata": {
        "id": "Kb2y9YkmD9iK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ue3b-dwEDp4y"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import nltk\n",
        "from typing import List\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.embeddings import CohereEmbeddings\n",
        "from langchain.vectorstores import Milvus\n",
        "from langchain.document_loaders import (\n",
        "    CSVLoader,\n",
        "    EverNoteLoader,\n",
        "    PDFMinerLoader,\n",
        "    TextLoader,\n",
        "    UnstructuredEmailLoader,\n",
        "    UnstructuredEPubLoader,\n",
        "    UnstructuredHTMLLoader,\n",
        "    UnstructuredMarkdownLoader,\n",
        "    UnstructuredODTLoader,\n",
        "    UnstructuredPowerPointLoader,\n",
        "    UnstructuredWordDocumentLoader,\n",
        ")\n",
        "from langchain.schema import Document as LangChainDocument\n",
        "from langchain.llms import Cohere\n",
        "from langchain.llms import Cohere\n",
        "from llama_index.llms import LangChainLLM\n",
        "from llama_index import (\n",
        "    GPTVectorStoreIndex,\n",
        "    GPTSimpleKeywordTableIndex,\n",
        "    ServiceContext,\n",
        "    StorageContext\n",
        ")\n",
        "from llama_index.indices.composability import ComposableGraph\n",
        "from llama_index.indices.query.query_transform.base import DecomposeQueryTransform\n",
        "from llama_index.query_engine.transform_query_engine import TransformQueryEngine\n",
        "from llama_index import ServiceContext, LLMPredictor\n",
        "from llama_index.llms import LangChainLLM\n",
        "from llama_index.langchain_helpers.text_splitter import TokenTextSplitter\n",
        "from llama_index.node_parser import SimpleNodeParser\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Define a custom loader that extends UnstructuredEmailLoader. This loader is used to handle emails without HTML content.\n",
        "class MyElmLoader(UnstructuredEmailLoader):\n",
        "    \"\"\"Wrapper to fallback to text/plain when default does not work\"\"\"\n",
        "    def load(self) -> List[Document]:\n",
        "        \"\"\"Wrapper adding fallback for elm without html\"\"\"\n",
        "        try:\n",
        "            try:\n",
        "                # Try to load the email as HTML\n",
        "                docs = UnstructuredEmailLoader.load(self)\n",
        "            except ValueError as e:\n",
        "                if 'text/html content not found in email' in str(e):\n",
        "                    # If HTML content is not found, try to load the email as plain text\n",
        "                    self.unstructured_kwargs[\"content_source\"] = \"text/plain\"\n",
        "                    docs = UnstructuredEmailLoader.load(self)\n",
        "                else:\n",
        "                    raise\n",
        "        except Exception as e:\n",
        "            # If any other exception occurs, add the file path to the exception message and raise it\n",
        "            raise type(e)(f\"{self.file_path}: {e}\") from e\n",
        "        return docs\n",
        "\n",
        "class Document(LangChainDocument):\n",
        "    def get_doc_id(self):\n",
        "        return self.title  # or another unique identifier\n",
        "\n",
        "# Map file extensions to their corresponding loaders and their arguments.\n",
        "LOADER_MAPPING = {\n",
        "    \".csv\": (CSVLoader, {}),\n",
        "    \".doc\": (UnstructuredWordDocumentLoader, {}),\n",
        "    \".docx\": (UnstructuredWordDocumentLoader, {}),\n",
        "    \".enex\": (EverNoteLoader, {}),\n",
        "    \".eml\": (MyElmLoader, {}),\n",
        "    \".epub\": (UnstructuredEPubLoader, {}),\n",
        "    \".html\": (UnstructuredHTMLLoader, {}),\n",
        "    \".md\": (UnstructuredMarkdownLoader, {}),\n",
        "    \".odt\": (UnstructuredODTLoader, {}),\n",
        "    \".pdf\": (PDFMinerLoader, {}),\n",
        "    \".ppt\": (UnstructuredPowerPointLoader, {}),\n",
        "    \".pptx\": (UnstructuredPowerPointLoader, {}),\n",
        "    \".txt\": (TextLoader, {\"encoding\": \"utf8\"}),\n",
        "}\n",
        "\n",
        "source_directory = '/Users/matthewsimon/Documents/GitHub/cohere/cohere_docs'\n",
        "processed_files = set()  # Using a set to avoid duplicates\n",
        "all_documents = []  # A list to collect all documents\n",
        "\n",
        "# Iterate over all files in the directory\n",
        "for filename in os.listdir(source_directory):\n",
        "    file_path = os.path.join(source_directory, filename)\n",
        "\n",
        "    # If the file is already processed, skip it\n",
        "    if file_path in processed_files:\n",
        "        continue\n",
        "\n",
        "    # Get the file extension\n",
        "    file_extension = os.path.splitext(filename)[1]\n",
        "\n",
        "    # Get the appropriate loader and its arguments from the LOADER_MAPPING dictionary\n",
        "    Loader, args = LOADER_MAPPING.get(file_extension, (None, None))\n",
        "\n",
        "    # If a loader was found\n",
        "    if Loader:\n",
        "        # Create an instance of the loader\n",
        "        loader = Loader(file_path, **args)\n",
        "\n",
        "        try:\n",
        "            # Load the file\n",
        "            docs = loader.load()\n",
        "\n",
        "            # Convert the loaded documents to your custom Document class\n",
        "            docs = [Document.from_other(doc) for doc in docs]\n",
        "\n",
        "            # If the file is successfully processed, add it to the processed_files\n",
        "            processed_files.add(file_path)\n",
        "\n",
        "            # Add the documents to the list of all documents\n",
        "            all_documents.extend(docs)\n",
        "        except Exception as e:\n",
        "            print(f\"Failed to process file {file_path}. Error: {e}. Skipping file.\")\n",
        "    else:\n",
        "        print(f\"No loader found for file with extension {file_extension}. Skipping file.\")\n",
        "\n",
        "# Now, you can process all_documents\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\n",
        "\n",
        "flattened_texts = []\n",
        "\n",
        "for document in tqdm(all_documents, desc=\"Splitting text\"):\n",
        "    # Extract the raw text from the document\n",
        "    raw_text = document.page_content  # Assuming document.page_content gives the raw text\n",
        "\n",
        "    # Split the raw text using the NLTK tokenizer\n",
        "    sentences = nltk.sent_tokenize(raw_text)\n",
        "\n",
        "    # Split the sentences into chunks using RecursiveCharacterTextSplitter\n",
        "    for sentence in sentences:\n",
        "        chunks = text_splitter.split_text(sentence)\n",
        "        flattened_texts.extend(chunks)\n",
        "\n",
        "embeddings = CohereEmbeddings(model=\"multilingual-22-12\")\n",
        "\n",
        "# Set up a vector store used to save the vector embeddings.\n",
        "connection_args = {\n",
        "    \"host\": \"localhost\",\n",
        "    \"port\": 19530,\n",
        "}\n",
        "\n",
        "vector_store = Milvus.from_texts(\n",
        "    tqdm(flattened_texts, desc=\"Creating embeddings\"),  # Corrected function call\n",
        "    embedding=embeddings,\n",
        "    connection_args=connection_args\n",
        ")\n",
        "\n",
        "os.environ[\"COHERE_API_KEY\"] = \"0imaNt4yu7l4MGgILVCXGpnrtJN4CBOFQTYuFsuY\"\n",
        "# Initialize the Cohere LLM\n",
        "llm = LangChainLLM(llm=Cohere())\n",
        "# Define service_context and storage_context\n",
        "\n",
        "service_context = ServiceContext.from_defaults(llm=llm)\n",
        "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
        "\n",
        "for index in document_indices.values():\n",
        "    query_engine = index.as_query_engine(service_context=service_context)\n",
        "    transform_extra_info = {'index_summary': index.index_struct.summary}\n",
        "    tranformed_query_engine = TransformQueryEngine(query_engine, decompose_transform, transform_extra_info=transform_extra_info)\n",
        "    custom_query_engines[index.index_id] = tranformed_query_engine\n",
        "\n",
        "# Build document index\n",
        "document_indices = {}\n",
        "index_summaries = {}\n",
        "for document in all_documents:\n",
        "    document_indices[document.title] = GPTVectorStoreIndex.from_documents([document], service_context=service_context, storage_context=storage_context)\n",
        "    # set summary text for document\n",
        "    index_summaries[document.title] = f\"Document: {document.title}\"\n",
        "\n",
        "graph = ComposableGraph.from_indices(\n",
        "    GPTSimpleKeywordTableIndex,\n",
        "    [index for _, index in document_indices.items()],\n",
        "    [summary for _, summary in index_summaries.items()],\n",
        "    max_keywords_per_chunk=50,\n",
        "    custom_query_engines=custom_query_engines,  # Add custom query engines to the graph\n",
        "    transform_extra_info = {'index_summary': index.index_struct.summary}\n",
        ")\n",
        "\n",
        "decompose_transform = DecomposeQueryTransform(\n",
        "    llm, verbose=True\n",
        ")\n",
        "\n",
        "custom_query_engines[graph.root_index.index_id] = graph.root_index.as_query_engine(\n",
        "    retriever_mode='simple',\n",
        "    response_mode='tree_summarize',\n",
        "    service_context=service_context\n",
        ")\n",
        "\n",
        "query_engine_decompose = graph.as_query_engine(\n",
        "    custom_query_engines=custom_query_engines,\n",
        ")\n",
        "\n",
        "decompose_transform = DecomposeQueryTransform(\n",
        "    llm, verbose=True\n",
        ")\n",
        "tranformed_query_engine = TransformQueryEngine(query_engine_decompose, decompose_transform, transform_extra_info=transform_extra_info)"
      ]
    }
  ]
}