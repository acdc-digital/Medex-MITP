{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Main function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import abstractmethod\n",
    "from typing import AsyncIterable, List\n",
    "\n",
    "from langchain.callbacks import AsyncIteratorCallbackHandler\n",
    "from langchain.callbacks.base import AsyncCallbackHandler\n",
    "from langchain.chains import ConversationalRetrievalChain, LLMChain\n",
    "from langchain.llms.base import LLM\n",
    "from logger import get_logger\n",
    "from models.settings import BrainSettings  # Importing settings related to the 'brain'\n",
    "from pydantic import BaseModel  # For data validation and settings management\n",
    "from utils.constants import streaming_compatible_models\n",
    "\n",
    "logger = get_logger(__name__)\n",
    "\n",
    "\n",
    "class BaseBrainPicking(BaseModel):\n",
    "    \"\"\"\n",
    "    Base Class for BrainPicking. Allows you to interact with LLMs (large language models)\n",
    "    Use this class to define abstract methods and methods and properties common to all classes.\n",
    "    \"\"\"\n",
    "\n",
    "    # Instantiate settings\n",
    "    brain_settings = BrainSettings()\n",
    "\n",
    "    # Default class attributes\n",
    "    model: str = None\n",
    "    temperature: float = 0.0\n",
    "    chat_id: str = None\n",
    "    brain_id: str = None\n",
    "    max_tokens: int = 256\n",
    "    user_openai_api_key: str = None\n",
    "    streaming: bool = False\n",
    "\n",
    "    openai_api_key: str = None\n",
    "    callbacks: List[AsyncCallbackHandler] = None\n",
    "\n",
    "    def _determine_api_key(self, openai_api_key, user_openai_api_key):\n",
    "        \"\"\"If user provided an API key, use it.\"\"\"\n",
    "        if user_openai_api_key is not None:\n",
    "            return user_openai_api_key\n",
    "        else:\n",
    "            return openai_api_key\n",
    "\n",
    "    def _determine_streaming(self, model: str, streaming: bool) -> bool:\n",
    "        \"\"\"If the model name allows for streaming and streaming is declared, set streaming to True.\"\"\"\n",
    "        if model in streaming_compatible_models and streaming:\n",
    "            return True\n",
    "        if model not in streaming_compatible_models and streaming:\n",
    "            logger.warning(\n",
    "                f\"Streaming is not compatible with {model}. Streaming will be set to False.\"\n",
    "            )\n",
    "            return False\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "    def _determine_callback_array(\n",
    "        self, streaming\n",
    "    ) -> List[AsyncIteratorCallbackHandler]:\n",
    "        \"\"\"If streaming is set, set the AsyncIteratorCallbackHandler as the only callback.\"\"\"\n",
    "        if streaming:\n",
    "            return [AsyncIteratorCallbackHandler]\n",
    "\n",
    "    def __init__(self, **data):\n",
    "        super().__init__(**data)\n",
    "\n",
    "        self.openai_api_key = self._determine_api_key(\n",
    "            self.brain_settings.openai_api_key, self.user_openai_api_key\n",
    "        )\n",
    "        self.streaming = self._determine_streaming(self.model, self.streaming)\n",
    "        self.callbacks = self._determine_callback_array(self.streaming)\n",
    "\n",
    "    class Config:\n",
    "        \"\"\"Configuration of the Pydantic Object\"\"\"\n",
    "\n",
    "        # Allowing arbitrary types for class validation\n",
    "        arbitrary_types_allowed = True\n",
    "\n",
    "        # the below methods define the names, arguments and return types for the most useful functions for the child classes. These should be overwritten if they are used.\n",
    "        @abstractmethod\n",
    "        def _create_llm(self, model, streaming=False, callbacks=None) -> LLM:\n",
    "            \"\"\"\n",
    "            Determine and construct the language model.\n",
    "            :param model: Language model name to be used.\n",
    "            :return: Language model instance\n",
    "\n",
    "            This method should take into account the following:\n",
    "            - Whether the model is streaming compatible\n",
    "            - Whether the model is private\n",
    "            - Whether the model should use an openai api key and use the _determine_api_key method\n",
    "            \"\"\"\n",
    "\n",
    "        @abstractmethod\n",
    "        def _create_question_chain(self, model) -> LLMChain:\n",
    "            \"\"\"\n",
    "            Determine and construct the question chain.\n",
    "            :param model: Language model name to be used.\n",
    "            :return: Question chain instance\n",
    "\n",
    "            This method should take into account the following:\n",
    "            - Which prompt to use (normally CONDENSE_QUESTION_PROMPT)\n",
    "            \"\"\"\n",
    "\n",
    "        @abstractmethod\n",
    "        def _create_doc_chain(self, model) -> LLMChain:\n",
    "            \"\"\"\n",
    "            Determine and construct the document chain.\n",
    "            :param model Language model name to be used.\n",
    "            :return: Document chain instance\n",
    "\n",
    "            This method should take into account the following:\n",
    "            - chain_type (normally \"stuff\")\n",
    "            - Whether the model is streaming compatible and/or streaming is set (determine_streaming).\n",
    "            \"\"\"\n",
    "\n",
    "        @abstractmethod\n",
    "        def _create_qa(\n",
    "            self, question_chain, document_chain\n",
    "        ) -> ConversationalRetrievalChain:\n",
    "            \"\"\"\n",
    "            Constructs a conversational retrieval chain .\n",
    "            :param question_chain\n",
    "            :param document_chain\n",
    "            :return: ConversationalRetrievalChain instance\n",
    "            \"\"\"\n",
    "\n",
    "        @abstractmethod\n",
    "        def _call_chain(self, chain, question, history) -> str:\n",
    "            \"\"\"\n",
    "            Call a chain with a given question and history.\n",
    "            :param chain: The chain eg QA (ConversationalRetrievalChain)\n",
    "            :param question: The user prompt\n",
    "            :param history: The chat history from DB\n",
    "            :return: The answer.\n",
    "            \"\"\"\n",
    "\n",
    "        async def _acall_chain(self, chain, question, history) -> str:\n",
    "            \"\"\"\n",
    "            Call a chain with a given question and history.\n",
    "            :param chain: The chain eg qa (ConversationalRetrievalChain)\n",
    "            :param question: The user prompt\n",
    "            :param history: The chat history from DB\n",
    "            :return: The answer.\n",
    "            \"\"\"\n",
    "            raise NotImplementedError(\n",
    "                \"Async generation not implemented for this BrainPicking Class.\"\n",
    "            )\n",
    "\n",
    "        @abstractmethod\n",
    "        def generate_answer(self, question: str) -> str:\n",
    "            \"\"\"\n",
    "            Generate an answer to a given question using QA Chain.\n",
    "            :param question: The question\n",
    "            :return: The generated answer.\n",
    "\n",
    "            This function should also call: _create_qa, get_chat_history and format_chat_history.\n",
    "            It should also update the chat_history in the DB.\n",
    "            \"\"\"\n",
    "\n",
    "        async def generate_stream(self, question: str) -> AsyncIterable:\n",
    "            \"\"\"\n",
    "            Generate a streaming answer to a given question using QA Chain.\n",
    "            :param question: The question\n",
    "            :return: An async iterable which generates the answer.\n",
    "\n",
    "            This function has to do some other things:\n",
    "            - Update the chat history in the DB with the chat details(chat_id, question) -> Return a message_id and timestamp\n",
    "            - Use the _acall_chain method inside create_task from asyncio to run the process on a child thread.\n",
    "            - Append each token to the chat_history object from the db and yield it from the function\n",
    "            - Append each token from the callback to an answer string -> Used to update chat history in DB (update_message_by_id)\n",
    "            \"\"\"\n",
    "            raise NotImplementedError(\n",
    "                \"Async generation not implemented for this BrainPicking Class.\"\n",
    "            )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Main (master)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import pypandoc\n",
    "import sentry_sdk\n",
    "from fastapi import FastAPI, HTTPException\n",
    "from fastapi.responses import JSONResponse\n",
    "from logger import get_logger\n",
    "from middlewares.cors import add_cors_middleware\n",
    "from routes.api_key_routes import api_key_router\n",
    "from routes.brain_routes import brain_router\n",
    "from routes.chat_routes import chat_router\n",
    "from routes.crawl_routes import crawl_router\n",
    "from routes.explore_routes import explore_router\n",
    "from routes.misc_routes import misc_router\n",
    "from routes.upload_routes import upload_router\n",
    "from routes.user_routes import user_router\n",
    "\n",
    "logger = get_logger(__name__)\n",
    "\n",
    "if os.getenv(\"SENTRY_DSN\"):\n",
    "    sentry_sdk.init(\n",
    "        dsn=os.getenv(\"SENTRY_DSN\"),\n",
    "        # Set traces_sample_rate to 1.0 to capture 100%\n",
    "        # of transactions for performance monitoring.\n",
    "        # We recommend adjusting this value in production,\n",
    "        traces_sample_rate=1.0,\n",
    "    )\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "add_cors_middleware(app)\n",
    "max_brain_size = os.getenv(\"MAX_BRAIN_SIZE\")\n",
    "max_brain_size_with_own_key = os.getenv(\"MAX_BRAIN_SIZE_WITH_KEY\", 209715200)\n",
    "\n",
    "\n",
    "@app.on_event(\"startup\")\n",
    "async def startup_event():\n",
    "    pypandoc.download_pandoc()\n",
    "\n",
    "\n",
    "app.include_router(brain_router)\n",
    "app.include_router(chat_router)\n",
    "app.include_router(crawl_router)\n",
    "app.include_router(explore_router)\n",
    "app.include_router(misc_router)\n",
    "app.include_router(upload_router)\n",
    "app.include_router(user_router)\n",
    "app.include_router(api_key_router)\n",
    "\n",
    "\n",
    "@app.exception_handler(HTTPException)\n",
    "async def http_exception_handler(_, exc):\n",
    "    return JSONResponse(\n",
    "        status_code=exc.status_code,\n",
    "        content={\"detail\": exc.detail},\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
