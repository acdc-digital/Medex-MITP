{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import tempfile\n",
    "import unicodedata\n",
    "\n",
    "import requests\n",
    "from langchain.document_loaders import GitLoader\n",
    "from pydantic import BaseModel\n",
    "\n",
    "\n",
    "class CrawlWebsite(BaseModel):\n",
    "    url: str\n",
    "    js: bool = False\n",
    "    depth: int = 1\n",
    "    max_pages: int = 100\n",
    "    max_time: int = 60\n",
    "\n",
    "    def _crawl(self, url):\n",
    "        response = requests.get(url)\n",
    "        if response.status_code == 200:\n",
    "            return response.text\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    def process(self):\n",
    "        content = self._crawl(self.url)\n",
    "\n",
    "        # Create a file\n",
    "        file_name = slugify(self.url) + \".html\"\n",
    "        temp_file_path = os.path.join(tempfile.gettempdir(), file_name)\n",
    "        with open(temp_file_path, \"w\") as temp_file:\n",
    "            temp_file.write(content)\n",
    "            # Process the file\n",
    "\n",
    "        if content:\n",
    "            return temp_file_path, file_name\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    def checkGithub(self):\n",
    "        if \"github.com\" in self.url:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "\n",
    "def slugify(text):\n",
    "    text = unicodedata.normalize(\"NFKD\", text).encode(\"ascii\", \"ignore\").decode(\"utf-8\")\n",
    "    text = re.sub(r\"[^\\w\\s-]\", \"\", text).strip().lower()\n",
    "    text = re.sub(r\"[-\\s]+\", \"-\", text)\n",
    "    return text"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
