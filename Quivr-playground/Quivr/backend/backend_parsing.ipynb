{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "from uuid import UUID\n",
    "\n",
    "from logger import get_logger\n",
    "from models.settings import common_dependencies\n",
    "from pydantic import BaseModel\n",
    "\n",
    "logger = get_logger(__name__)\n",
    "\n",
    "\n",
    "class User(BaseModel):\n",
    "    id: UUID\n",
    "    email: Optional[str]\n",
    "    user_openai_api_key: Optional[str] = None\n",
    "    requests_count: int = 0\n",
    "\n",
    "    # [TODO] Rename the user table and its references to 'user_usage'\n",
    "    def create_user(self, date):\n",
    "        commons = common_dependencies()\n",
    "        logger.info(f\"New user entry in db document for user {self.email}\")\n",
    "\n",
    "        return (\n",
    "            commons[\"supabase\"]\n",
    "            .table(\"users\")\n",
    "            .insert(\n",
    "                {\n",
    "                    \"user_id\": self.id,\n",
    "                    \"email\": self.email,\n",
    "                    \"date\": date,\n",
    "                    \"requests_count\": 1,\n",
    "                }\n",
    "            )\n",
    "            .execute()\n",
    "        )\n",
    "\n",
    "    def get_user_request_stats(self):\n",
    "        commons = common_dependencies()\n",
    "        requests_stats = (\n",
    "            commons[\"supabase\"]\n",
    "            .from_(\"users\")\n",
    "            .select(\"*\")\n",
    "            .filter(\"user_id\", \"eq\", self.id)\n",
    "            .execute()\n",
    "        )\n",
    "        return requests_stats.data\n",
    "\n",
    "    def fetch_user_requests_count(self, date):\n",
    "        commons = common_dependencies()\n",
    "        response = (\n",
    "            commons[\"supabase\"]\n",
    "            .from_(\"users\")\n",
    "            .select(\"*\")\n",
    "            .filter(\"user_id\", \"eq\", self.id)\n",
    "            .filter(\"date\", \"eq\", date)\n",
    "            .execute()\n",
    "        )\n",
    "        userItem = next(iter(response.data or []), {\"requests_count\": 0})\n",
    "\n",
    "        return userItem[\"requests_count\"]\n",
    "\n",
    "    def increment_user_request_count(self, date):\n",
    "        commons = common_dependencies()\n",
    "        requests_count = self.fetch_user_requests_count(date) + 1\n",
    "        logger.info(f\"User {self.email} request count updated to {requests_count}\")\n",
    "        commons[\"supabase\"].table(\"users\").update(\n",
    "            {\"requests_count\": requests_count}\n",
    "        ).match({\"user_id\": self.id, \"date\": date}).execute()\n",
    "        self.requests_count = requests_count"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Common"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "from langchain.schema import Document\n",
    "from models.brains import Brain\n",
    "from models.files import File\n",
    "from models.settings import CommonsDep\n",
    "from utils.vectors import Neurons\n",
    "\n",
    "\n",
    "async def process_file(\n",
    "    commons: CommonsDep,\n",
    "    file: File,\n",
    "    loader_class,\n",
    "    enable_summarization,\n",
    "    brain_id,\n",
    "    user_openai_api_key,\n",
    "):\n",
    "    dateshort = time.strftime(\"%Y%m%d\")\n",
    "\n",
    "    file.compute_documents(loader_class)\n",
    "\n",
    "    for doc in file.documents:  # pyright: ignore reportPrivateUsage=none\n",
    "        metadata = {\n",
    "            \"file_sha1\": file.file_sha1,\n",
    "            \"file_size\": file.file_size,\n",
    "            \"file_name\": file.file_name,\n",
    "            \"chunk_size\": file.chunk_size,\n",
    "            \"chunk_overlap\": file.chunk_overlap,\n",
    "            \"date\": dateshort,\n",
    "            \"summarization\": \"true\" if enable_summarization else \"false\",\n",
    "        }\n",
    "        doc_with_metadata = Document(page_content=doc.page_content, metadata=metadata)\n",
    "\n",
    "        neurons = Neurons(commons=commons)\n",
    "        created_vector = neurons.create_vector(doc_with_metadata, user_openai_api_key)\n",
    "        # add_usage(stats_db, \"embedding\", \"audio\", metadata={\"file_name\": file_meta_name,\"file_type\": \".txt\", \"chunk_size\": chunk_size, \"chunk_overlap\": chunk_overlap})\n",
    "\n",
    "        created_vector_id = created_vector[0]  # pyright: ignore reportPrivateUsage=none\n",
    "\n",
    "        brain = Brain(id=brain_id)\n",
    "        brain.create_brain_vector(created_vector_id, file.file_sha1)\n",
    "\n",
    "    return"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import CSVLoader\n",
    "from models.files import File\n",
    "from models.settings import CommonsDep\n",
    "\n",
    "from .common import process_file\n",
    "\n",
    "\n",
    "def process_csv(\n",
    "    commons: CommonsDep,\n",
    "    file: File,\n",
    "    enable_summarization,\n",
    "    brain_id,\n",
    "    user_openai_api_key,\n",
    "):\n",
    "    return process_file(\n",
    "        commons,\n",
    "        file,\n",
    "        CSVLoader,\n",
    "        enable_summarization,\n",
    "        brain_id,\n",
    "        user_openai_api_key,\n",
    "    )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Docx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import Docx2txtLoader\n",
    "from models.files import File\n",
    "from models.settings import CommonsDep\n",
    "\n",
    "from .common import process_file\n",
    "\n",
    "\n",
    "def process_docx(commons: CommonsDep, file: File, enable_summarization, brain_id, user_openai_api_key):\n",
    "    return process_file(commons, file, Docx2txtLoader, enable_summarization, brain_id, user_openai_api_key)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Epub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders.epub import UnstructuredEPubLoader\n",
    "from models.files import File\n",
    "from models.settings import CommonsDep\n",
    "\n",
    "from .common import process_file\n",
    "\n",
    "\n",
    "def process_epub(commons: CommonsDep, file: File, enable_summarization, brain_id, user_openai_api_key):\n",
    "    return process_file(commons, file, UnstructuredEPubLoader, enable_summarization, brain_id, user_openai_api_key)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Guthub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "\n",
    "from langchain.document_loaders import GitLoader\n",
    "from langchain.schema import Document\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from models.brains import Brain\n",
    "from models.files import File\n",
    "from models.settings import CommonsDep\n",
    "from utils.file import compute_sha1_from_content\n",
    "from utils.vectors import Neurons\n",
    "\n",
    "\n",
    "async def process_github(\n",
    "    commons: CommonsDep,  # pyright: ignore reportPrivateUsage=none\n",
    "    repo,\n",
    "    enable_summarization,\n",
    "    brain_id,\n",
    "    user_openai_api_key,\n",
    "):\n",
    "    random_dir_name = os.urandom(16).hex()\n",
    "    dateshort = time.strftime(\"%Y%m%d\")\n",
    "    loader = GitLoader(\n",
    "        clone_url=repo,\n",
    "        repo_path=\"/tmp/\" + random_dir_name,\n",
    "    )\n",
    "    documents = loader.load()\n",
    "    os.system(\"rm -rf /tmp/\" + random_dir_name)\n",
    "\n",
    "    chunk_size = 500\n",
    "    chunk_overlap = 0\n",
    "    text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "        chunk_size=chunk_size, chunk_overlap=chunk_overlap\n",
    "    )\n",
    "\n",
    "    documents = text_splitter.split_documents(documents)\n",
    "    print(documents[:1])\n",
    "\n",
    "    for doc in documents:\n",
    "        if doc.metadata[\"file_type\"] in [\n",
    "            \".pyc\",\n",
    "            \".png\",\n",
    "            \".svg\",\n",
    "            \".env\",\n",
    "            \".lock\",\n",
    "            \".gitignore\",\n",
    "            \".gitmodules\",\n",
    "            \".gitattributes\",\n",
    "            \".gitkeep\",\n",
    "            \".git\",\n",
    "            \".json\",\n",
    "        ]:\n",
    "            continue\n",
    "        metadata = {\n",
    "            \"file_sha1\": compute_sha1_from_content(doc.page_content.encode(\"utf-8\")),\n",
    "            \"file_size\": len(doc.page_content) * 8,\n",
    "            \"file_name\": doc.metadata[\"file_name\"],\n",
    "            \"chunk_size\": chunk_size,\n",
    "            \"chunk_overlap\": chunk_overlap,\n",
    "            \"date\": dateshort,\n",
    "            \"summarization\": \"true\" if enable_summarization else \"false\",\n",
    "        }\n",
    "        doc_with_metadata = Document(page_content=doc.page_content, metadata=metadata)\n",
    "\n",
    "        file = File(\n",
    "            file_sha1=compute_sha1_from_content(doc.page_content.encode(\"utf-8\"))\n",
    "        )\n",
    "\n",
    "        file_exists = file.file_already_exists()\n",
    "\n",
    "        if not file_exists:\n",
    "            print(f\"Creating entry for file {file.file_sha1} in vectors...\")\n",
    "            neurons = Neurons(commons=commons)\n",
    "            created_vector = neurons.create_vector(\n",
    "                doc_with_metadata, user_openai_api_key\n",
    "            )\n",
    "            print(\"Created vector sids \", created_vector)\n",
    "            print(\"Created vector for \", doc.metadata[\"file_name\"])\n",
    "\n",
    "        file_exists_in_brain = file.file_already_exists_in_brain(brain_id)\n",
    "\n",
    "        if not file_exists_in_brain:\n",
    "            file.add_file_to_brain(brain_id)  # pyright: ignore reportPrivateUsage=none\n",
    "            brain = Brain(id=brain_id)\n",
    "            file.link_file_to_brain(brain)\n",
    "    return {\n",
    "        \"message\": f\"âœ… Github with {len(documents)} files has been uploaded.\",\n",
    "        \"type\": \"success\",\n",
    "    }"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import unicodedata\n",
    "\n",
    "import requests\n",
    "from langchain.document_loaders import UnstructuredHTMLLoader\n",
    "from models.files import File\n",
    "from models.settings import CommonsDep\n",
    "\n",
    "from .common import process_file\n",
    "\n",
    "\n",
    "def process_html(commons: CommonsDep, file: File, enable_summarization, brain_id, user_openai_api_key):\n",
    "    return process_file(commons, file, UnstructuredHTMLLoader,  enable_summarization, brain_id, user_openai_api_key)\n",
    "\n",
    "\n",
    "def get_html(url):\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        return response.text\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "\n",
    "def slugify(text):\n",
    "    text = unicodedata.normalize('NFKD', text).encode(\n",
    "        'ascii', 'ignore').decode('utf-8')\n",
    "    text = re.sub(r'[^\\w\\s-]', '', text).strip().lower()\n",
    "    text = re.sub(r'[-\\s]+', '-', text)\n",
    "    return text"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. Markdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import UnstructuredMarkdownLoader\n",
    "from models.files import File\n",
    "from models.settings import CommonsDep\n",
    "\n",
    "from .common import process_file\n",
    "\n",
    "\n",
    "def process_markdown(commons: CommonsDep, file: File, enable_summarization, brain_id, user_openai_api_key):\n",
    "    return process_file(commons, file, UnstructuredMarkdownLoader, enable_summarization, brain_id, user_openai_api_key)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9. Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import NotebookLoader\n",
    "from models.files import File\n",
    "from models.settings import CommonsDep\n",
    "\n",
    "from .common import process_file\n",
    "\n",
    "\n",
    "def process_ipnyb(commons: CommonsDep, file: File, enable_summarization, brain_id, user_openai_api_key):\n",
    "    return process_file(commons, file, NotebookLoader, enable_summarization, brain_id, user_openai_api_key)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10. Odt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyMuPDFLoader\n",
    "from models.files import File\n",
    "from models.settings import CommonsDep\n",
    "\n",
    "from .common import process_file\n",
    "\n",
    "\n",
    "def process_odt(commons: CommonsDep, file: File, enable_summarization, brain_id, user_openai_api_key):\n",
    "    return process_file(commons, file, PyMuPDFLoader, enable_summarization, brain_id, user_openai_api_key)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "11. Pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyMuPDFLoader\n",
    "from models.files import File\n",
    "from models.settings import CommonsDep\n",
    "\n",
    "from .common import process_file\n",
    "\n",
    "\n",
    "def process_pdf(commons: CommonsDep, file: File, enable_summarization, brain_id, user_openai_api_key):\n",
    "    return process_file(commons, file, PyMuPDFLoader, enable_summarization, brain_id, user_openai_api_key)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "12. Powerpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import UnstructuredPowerPointLoader\n",
    "from models.files import File\n",
    "from models.settings import CommonsDep\n",
    "\n",
    "from .common import process_file\n",
    "\n",
    "\n",
    "def process_powerpoint(commons: CommonsDep, file: File, enable_summarization, brain_id, user_openai_api_key):\n",
    "    return process_file(commons, file, UnstructuredPowerPointLoader, enable_summarization, brain_id, user_openai_api_key)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "13. Txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import TextLoader\n",
    "from models.files import File\n",
    "from models.settings import CommonsDep\n",
    "\n",
    "from .common import process_file\n",
    "\n",
    "\n",
    "async def process_txt(commons: CommonsDep, file: File, enable_summarization, brain_id, user_openai_api_key):\n",
    "    return await process_file(commons, file, TextLoader, enable_summarization, brain_id,user_openai_api_key)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
