{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Annotated\n",
    "\n",
    "from fastapi import Depends\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from pydantic import BaseSettings\n",
    "from supabase.client import Client, create_client\n",
    "from vectorstore.supabase import SupabaseVectorStore\n",
    "\n",
    "\n",
    "class BrainSettings(BaseSettings):\n",
    "    openai_api_key: str\n",
    "    anthropic_api_key: str\n",
    "    supabase_url: str\n",
    "    supabase_service_key: str\n",
    "\n",
    "\n",
    "class LLMSettings(BaseSettings):\n",
    "    private: bool = False\n",
    "    model_path: str = \"gpt2\"\n",
    "    model_n_ctx: int = 1000\n",
    "    model_n_batch: int = 8\n",
    "\n",
    "\n",
    "def common_dependencies() -> dict:\n",
    "    settings = BrainSettings()  # pyright: ignore reportPrivateUsage=none\n",
    "    embeddings = OpenAIEmbeddings(\n",
    "        openai_api_key=settings.openai_api_key\n",
    "    )  # pyright: ignore reportPrivateUsage=none\n",
    "    supabase_client: Client = create_client(\n",
    "        settings.supabase_url, settings.supabase_service_key\n",
    "    )\n",
    "    documents_vector_store = SupabaseVectorStore(\n",
    "        supabase_client, embeddings, table_name=\"vectors\"\n",
    "    )\n",
    "    summaries_vector_store = SupabaseVectorStore(\n",
    "        supabase_client, embeddings, table_name=\"summaries\"\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        \"supabase\": supabase_client,\n",
    "        \"embeddings\": embeddings,\n",
    "        \"documents_vector_store\": documents_vector_store,\n",
    "        \"summaries_vector_store\": summaries_vector_store,\n",
    "    }\n",
    "\n",
    "\n",
    "CommonsDep = Annotated[dict, Depends(common_dependencies)]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "/* Instructions:\n",
    "1. in .backend/supabase folder, create .env file with BEEHIIV_PUBLICATION_ID and BEEHIIV_API_KEY variables\n",
    "2. cd into .backend\n",
    "--- for the rest of these steps you will need your supabase project id which can be found in your console url: https://supabase.com/dashboard/project/<projectId> ---\n",
    "3. run `supabase secrets set --env-file ./supabase/.env` to set the environment variables\n",
    "4. run `supabase functions deploy add-new-email` to deploy the function\n",
    "5. in the supabase console go to Database/Webhook and create new and point it to the edge function 'add-new-email'. You will have to add a new header Authorization: Bearer ${anon public key from Settings/API} to the webhook.\n",
    "*/\n",
    "\n",
    "import { serve } from \"https://deno.land/std@0.168.0/http/server.ts\";\n",
    "\n",
    "const publicationId = Deno.env.get(\"BEEHIIV_PUBLICATION_ID\");\n",
    "const apiKey = Deno.env.get(\"BEEHIIV_API_KEY\");\n",
    "\n",
    "const url = `https://api.beehiiv.com/v2/publications/${publicationId}/subscriptions`;\n",
    "\n",
    "interface WebhookPayload {\n",
    "  type: \"INSERT\" | \"UPDATE\" | \"DELETE\";\n",
    "  table: string;\n",
    "  record: {\n",
    "    id: string;\n",
    "    aud: string;\n",
    "    role: string;\n",
    "    email: string;\n",
    "    phone: null;\n",
    "    created_at: string;\n",
    "  };\n",
    "}\n",
    "\n",
    "serve(\n",
    "  async (req: { json: () => WebhookPayload | PromiseLike<WebhookPayload> }) => {\n",
    "    if (!publicationId || !apiKey) {\n",
    "      throw new Error(\"Missing required environment variables\");\n",
    "    }\n",
    "\n",
    "    const payload: WebhookPayload = await req.json();\n",
    "\n",
    "    if (payload.record.email) {\n",
    "      const requestBody = {\n",
    "        email: payload.record.email,\n",
    "        send_welcome_email: false,\n",
    "        utm_source: \"quivr\",\n",
    "        utm_medium: \"organic\",\n",
    "        referring_site: \"https://quivr.app\",\n",
    "      };\n",
    "\n",
    "      const response = await fetch(url, {\n",
    "        method: \"POST\",\n",
    "        headers: {\n",
    "          \"Content-Type\": \"application/json\",\n",
    "          Authorization: `Bearer ${apiKey}`,\n",
    "          Accept: \"application/json\",\n",
    "        },\n",
    "        body: JSON.stringify(requestBody),\n",
    "      });\n",
    "\n",
    "      if (!response.ok) {\n",
    "        throw new Error(\n",
    "          `Error adding email to Beehiiv: ${JSON.stringify(response)}`\n",
    "        );\n",
    "      }\n",
    "\n",
    "      const responseBody = await response.json();\n",
    "      return new Response(JSON.stringify(responseBody), {\n",
    "        status: response.status,\n",
    "        headers: { \"Content-Type\": \"application/json\" },\n",
    "      });\n",
    "    }\n",
    "\n",
    "    throw new Error(\n",
    "      `No email address found in payload: ${JSON.stringify(payload)}`\n",
    "    );\n",
    "  }\n",
    ");"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. File config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hashlib\n",
    "\n",
    "from fastapi import UploadFile\n",
    "\n",
    "\n",
    "def convert_bytes(bytes, precision=2):\n",
    "    \"\"\"Converts bytes into a human-friendly format.\"\"\"\n",
    "    abbreviations = [\"B\", \"KB\", \"MB\"]\n",
    "    if bytes <= 0:\n",
    "        return \"0 B\"\n",
    "    size = bytes\n",
    "    index = 0\n",
    "    while size >= 1024 and index < len(abbreviations) - 1:\n",
    "        size /= 1024\n",
    "        index += 1\n",
    "    return f\"{size:.{precision}f} {abbreviations[index]}\"\n",
    "\n",
    "\n",
    "def get_file_size(file: UploadFile):\n",
    "    # move the cursor to the end of the file\n",
    "    file.file._file.seek(0, 2)  # pyright: ignore reportPrivateUsage=none\n",
    "    file_size = (\n",
    "        file.file._file.tell()  # pyright: ignore reportPrivateUsage=none\n",
    "    )  # Getting the size of the file\n",
    "    # move the cursor back to the beginning of the file\n",
    "    file.file.seek(0)\n",
    "\n",
    "    return file_size\n",
    "\n",
    "\n",
    "def compute_sha1_from_file(file_path):\n",
    "    with open(file_path, \"rb\") as file:\n",
    "        bytes = file.read()\n",
    "        readable_hash = compute_sha1_from_content(bytes)\n",
    "    return readable_hash\n",
    "\n",
    "\n",
    "def compute_sha1_from_content(content):\n",
    "    readable_hash = hashlib.sha1(content).hexdigest()\n",
    "    return readable_hash"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Processors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.brains import Brain\n",
    "from models.files import File\n",
    "from models.settings import CommonsDep\n",
    "from parsers.audio import process_audio\n",
    "from parsers.csv import process_csv\n",
    "from parsers.docx import process_docx\n",
    "from parsers.epub import process_epub\n",
    "from parsers.html import process_html\n",
    "from parsers.markdown import process_markdown\n",
    "from parsers.notebook import process_ipnyb\n",
    "from parsers.odt import process_odt\n",
    "from parsers.pdf import process_pdf\n",
    "from parsers.powerpoint import process_powerpoint\n",
    "from parsers.txt import process_txt\n",
    "\n",
    "file_processors = {\n",
    "    \".txt\": process_txt,\n",
    "    \".csv\": process_csv,\n",
    "    \".md\": process_markdown,\n",
    "    \".markdown\": process_markdown,\n",
    "    \".m4a\": process_audio,\n",
    "    \".mp3\": process_audio,\n",
    "    \".webm\": process_audio,\n",
    "    \".mp4\": process_audio,\n",
    "    \".mpga\": process_audio,\n",
    "    \".wav\": process_audio,\n",
    "    \".mpeg\": process_audio,\n",
    "    \".pdf\": process_pdf,\n",
    "    \".html\": process_html,\n",
    "    \".pptx\": process_powerpoint,\n",
    "    \".docx\": process_docx,\n",
    "    \".odt\": process_odt,\n",
    "    \".epub\": process_epub,\n",
    "    \".ipynb\": process_ipnyb,\n",
    "}\n",
    "\n",
    "\n",
    "def create_response(message, type):\n",
    "    return {\"message\": message, \"type\": type}\n",
    "\n",
    "\n",
    "async def filter_file(\n",
    "    commons: CommonsDep,\n",
    "    file: File,\n",
    "    enable_summarization: bool,\n",
    "    brain_id,\n",
    "    openai_api_key,\n",
    "):\n",
    "    await file.compute_file_sha1()\n",
    "\n",
    "    print(\"file sha1\", file.file_sha1)\n",
    "    file_exists = file.file_already_exists()\n",
    "    file_exists_in_brain = file.file_already_exists_in_brain(brain_id)\n",
    "\n",
    "    if file_exists_in_brain:\n",
    "        return create_response(\n",
    "            f\"🤔 {file.file.filename} already exists in brain {brain_id}.\",  # pyright: ignore reportPrivateUsage=none\n",
    "            \"warning\",\n",
    "        )\n",
    "    elif file.file_is_empty():\n",
    "        return create_response(\n",
    "            f\"❌ {file.file.filename} is empty.\",  # pyright: ignore reportPrivateUsage=none\n",
    "            \"error\",  # pyright: ignore reportPrivateUsage=none\n",
    "        )\n",
    "    elif file_exists:\n",
    "        file.link_file_to_brain(brain=Brain(id=brain_id))\n",
    "        return create_response(\n",
    "            f\"✅ {file.file.filename} has been uploaded to brain {brain_id}.\",  # pyright: ignore reportPrivateUsage=none\n",
    "            \"success\",\n",
    "        )\n",
    "\n",
    "    if file.file_extension in file_processors:\n",
    "        try:\n",
    "            await file_processors[file.file_extension](\n",
    "                commons, file, enable_summarization, brain_id, openai_api_key\n",
    "            )\n",
    "            return create_response(\n",
    "                f\"✅ {file.file.filename} has been uploaded to brain {brain_id}.\",  # pyright: ignore reportPrivateUsage=none\n",
    "                \"success\",\n",
    "            )\n",
    "        except Exception as e:\n",
    "            # Add more specific exceptions as needed.\n",
    "            print(f\"Error processing file: {e}\")\n",
    "            return create_response(\n",
    "                f\"⚠️ An error occurred while processing {file.file.filename}.\",  # pyright: ignore reportPrivateUsage=none\n",
    "                \"error\",\n",
    "            )\n",
    "\n",
    "    return create_response(\n",
    "        f\"❌ {file.file.filename} is not supported.\",  # pyright: ignore reportPrivateUsage=none\n",
    "        \"error\",\n",
    "    )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Vector config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from typing import List\n",
    "\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.schema import Document\n",
    "from llm.utils.summarization import llm_summerize\n",
    "from logger import get_logger\n",
    "from models.settings import BrainSettings, CommonsDep, common_dependencies\n",
    "from pydantic import BaseModel\n",
    "\n",
    "logger = get_logger(__name__)\n",
    "\n",
    "\n",
    "class Neurons(BaseModel):\n",
    "    commons: CommonsDep\n",
    "    settings = BrainSettings()  # pyright: ignore reportPrivateUsage=none\n",
    "\n",
    "    def create_vector(self, doc, user_openai_api_key=None):\n",
    "        logger.info(\"Creating vector for document\")\n",
    "        logger.info(f\"Document: {doc}\")\n",
    "        if user_openai_api_key:\n",
    "            self.commons[\"documents_vector_store\"]._embedding = OpenAIEmbeddings(\n",
    "                openai_api_key=user_openai_api_key\n",
    "            )  # pyright: ignore reportPrivateUsage=none\n",
    "        try:\n",
    "            sids = self.commons[\"documents_vector_store\"].add_documents([doc])\n",
    "            if sids and len(sids) > 0:\n",
    "                return sids\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error creating vector for document {e}\")\n",
    "\n",
    "    def create_embedding(self, content):\n",
    "        return self.commons[\"embeddings\"].embed_query(content)\n",
    "\n",
    "    def similarity_search(self, query, table=\"match_summaries\", top_k=5, threshold=0.5):\n",
    "        query_embedding = self.create_embedding(query)\n",
    "        summaries = (\n",
    "            self.commons[\"supabase\"]\n",
    "            .rpc(\n",
    "                table,\n",
    "                {\n",
    "                    \"query_embedding\": query_embedding,\n",
    "                    \"match_count\": top_k,\n",
    "                    \"match_threshold\": threshold,\n",
    "                },\n",
    "            )\n",
    "            .execute()\n",
    "        )\n",
    "        return summaries.data\n",
    "\n",
    "\n",
    "def create_summary(commons: CommonsDep, document_id, content, metadata):\n",
    "    logger.info(f\"Summarizing document {content[:100]}\")\n",
    "    summary = llm_summerize(content)\n",
    "    logger.info(f\"Summary: {summary}\")\n",
    "    metadata[\"document_id\"] = document_id\n",
    "    summary_doc_with_metadata = Document(page_content=summary, metadata=metadata)\n",
    "    sids = commons[\"summaries_vector_store\"].add_documents([summary_doc_with_metadata])\n",
    "    if sids and len(sids) > 0:\n",
    "        commons[\"supabase\"].table(\"summaries\").update(\n",
    "            {\"document_id\": document_id}\n",
    "        ).match({\"id\": sids[0]}).execute()\n",
    "\n",
    "\n",
    "def error_callback(exception):\n",
    "    print(\"An exception occurred:\", exception)\n",
    "\n",
    "\n",
    "def process_batch(batch_ids):\n",
    "    commons = common_dependencies()\n",
    "    if len(batch_ids) == 1:\n",
    "        return (\n",
    "            commons[\"supabase\"]\n",
    "            .table(\"vectors\")\n",
    "            .select(\n",
    "                \"name:metadata->>file_name, size:metadata->>file_size\",\n",
    "                count=\"exact\",\n",
    "            )\n",
    "            .filter(\"id\", \"eq\", batch_ids[0])\n",
    "            .execute()\n",
    "        ).data\n",
    "    else:\n",
    "        return (\n",
    "            commons[\"supabase\"]\n",
    "            .table(\"vectors\")\n",
    "            .select(\n",
    "                \"name:metadata->>file_name, size:metadata->>file_size\",\n",
    "                count=\"exact\",\n",
    "            )\n",
    "            .filter(\"id\", \"in\", tuple(batch_ids))\n",
    "            .execute()\n",
    "        ).data\n",
    "\n",
    "\n",
    "def get_unique_files_from_vector_ids(vectors_ids: List[int]):\n",
    "    # Move into Vectors class\n",
    "    \"\"\"\n",
    "    Retrieve unique user data vectors.\n",
    "    \"\"\"\n",
    "    print(\"vectors_ids\", vectors_ids)\n",
    "\n",
    "    # constants\n",
    "    BATCH_SIZE = 5\n",
    "\n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        futures = []\n",
    "        for i in range(0, len(vectors_ids), BATCH_SIZE):\n",
    "            batch_ids = vectors_ids[i : i + BATCH_SIZE]\n",
    "            future = executor.submit(process_batch, batch_ids)\n",
    "            futures.append(future)\n",
    "\n",
    "        # Retrieve the results\n",
    "        vectors_responses = [future.result() for future in futures]\n",
    "\n",
    "    documents = [item for sublist in vectors_responses for item in sublist]\n",
    "    print(\"document\", documents)\n",
    "    unique_files = [dict(t) for t in set(tuple(d.items()) for d in documents)]\n",
    "    return unique_files"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Vectorstore "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, List\n",
    "\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores import SupabaseVectorStore\n",
    "from supabase.client import Client\n",
    "\n",
    "\n",
    "class CustomSupabaseVectorStore(SupabaseVectorStore):\n",
    "    \"\"\"A custom vector store that uses the match_vectors table instead of the vectors table.\"\"\"\n",
    "\n",
    "    brain_id: str = \"none\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        client: Client,\n",
    "        embedding: OpenAIEmbeddings,\n",
    "        table_name: str,\n",
    "        brain_id: str = \"none\",\n",
    "    ):\n",
    "        super().__init__(client, embedding, table_name)\n",
    "        self.brain_id = brain_id\n",
    "\n",
    "    def similarity_search(\n",
    "        self,\n",
    "        query: str,\n",
    "        table: str = \"match_vectors\",\n",
    "        k: int = 6,\n",
    "        threshold: float = 0.5,\n",
    "        **kwargs: Any\n",
    "    ) -> List[Document]:\n",
    "        vectors = self._embedding.embed_documents([query])\n",
    "        query_embedding = vectors[0]\n",
    "        res = self._client.rpc(\n",
    "            table,\n",
    "            {\n",
    "                \"query_embedding\": query_embedding,\n",
    "                \"match_count\": k,\n",
    "                \"p_brain_id\": str(self.brain_id),\n",
    "            },\n",
    "        ).execute()\n",
    "\n",
    "        match_result = [\n",
    "            (\n",
    "                Document(\n",
    "                    metadata=search.get(\"metadata\", {}),  # type: ignore\n",
    "                    page_content=search.get(\"content\", \"\"),\n",
    "                ),\n",
    "                search.get(\"similarity\", 0.0),\n",
    "            )\n",
    "            for search in res.data\n",
    "            if search.get(\"content\")\n",
    "        ]\n",
    "\n",
    "        documents = [doc for doc, _ in match_result]\n",
    "\n",
    "        return documents"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
