{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-1. Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_function_compatible_models = [\n",
    "    \"gpt-3.5-turbo-0613\",\n",
    "    \"gpt-4-0613\",\n",
    "]\n",
    "\n",
    "streaming_compatible_models = [\"gpt-3.5-turbo, gpt4all-j-1.3\"]\n",
    "\n",
    "private_models = [\"gpt4all-j-1.3\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0. OpenAi configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import json\n",
    "from typing import AsyncIterable, Awaitable\n",
    "\n",
    "from langchain.chains import ConversationalRetrievalChain, LLMChain\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.llms.base import LLM\n",
    "from logger import get_logger\n",
    "from models.chat import ChatHistory\n",
    "from repository.chat.format_chat_history import format_chat_history\n",
    "from repository.chat.get_chat_history import get_chat_history\n",
    "from repository.chat.update_chat_history import update_chat_history\n",
    "from repository.chat.update_message_by_id import update_message_by_id\n",
    "from supabase import Client, create_client\n",
    "from vectorstore.supabase import (\n",
    "    CustomSupabaseVectorStore,\n",
    ")  # Custom class for handling vector storage with Supabase\n",
    "\n",
    "from .base import BaseBrainPicking\n",
    "from .prompts.CONDENSE_PROMPT import CONDENSE_QUESTION_PROMPT\n",
    "\n",
    "logger = get_logger(__name__)\n",
    "\n",
    "\n",
    "class OpenAIBrainPicking(BaseBrainPicking):\n",
    "    \"\"\"\n",
    "    Main class for the OpenAI Brain Picking functionality.\n",
    "    It allows to initialize a Chat model, generate questions and retrieve answers using ConversationalRetrievalChain.\n",
    "    \"\"\"\n",
    "\n",
    "    # Default class attributes\n",
    "    model: str = \"gpt-3.5-turbo\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        model: str,\n",
    "        brain_id: str,\n",
    "        temperature: float,\n",
    "        chat_id: str,\n",
    "        max_tokens: int,\n",
    "        user_openai_api_key: str,\n",
    "        streaming: bool = False,\n",
    "    ) -> \"OpenAIBrainPicking\":\n",
    "        \"\"\"\n",
    "        Initialize the BrainPicking class by setting embeddings, supabase client, vector store, language model and chains.\n",
    "        :return: OpenAIBrainPicking instance\n",
    "        \"\"\"\n",
    "        super().__init__(\n",
    "            model=model,\n",
    "            brain_id=brain_id,\n",
    "            chat_id=chat_id,\n",
    "            max_tokens=max_tokens,\n",
    "            temperature=temperature,\n",
    "            user_openai_api_key=user_openai_api_key,\n",
    "            streaming=streaming,\n",
    "        )\n",
    "\n",
    "    @property\n",
    "    def embeddings(self) -> OpenAIEmbeddings:\n",
    "        return OpenAIEmbeddings(openai_api_key=self.openai_api_key)\n",
    "\n",
    "    @property\n",
    "    def supabase_client(self) -> Client:\n",
    "        return create_client(\n",
    "            self.brain_settings.supabase_url, self.brain_settings.supabase_service_key\n",
    "        )\n",
    "\n",
    "    @property\n",
    "    def vector_store(self) -> CustomSupabaseVectorStore:\n",
    "        return CustomSupabaseVectorStore(\n",
    "            self.supabase_client,\n",
    "            self.embeddings,\n",
    "            table_name=\"vectors\",\n",
    "            brain_id=self.brain_id,\n",
    "        )\n",
    "\n",
    "    @property\n",
    "    def question_llm(self) -> LLM:\n",
    "        return self._create_llm(model=self.model, streaming=False)\n",
    "\n",
    "    @property\n",
    "    def doc_llm(self) -> LLM:\n",
    "        return self._create_llm(\n",
    "            model=self.model, streaming=self.streaming, callbacks=self.callbacks\n",
    "        )\n",
    "\n",
    "    @property\n",
    "    def question_generator(self) -> LLMChain:\n",
    "        return LLMChain(llm=self.question_llm, prompt=CONDENSE_QUESTION_PROMPT)\n",
    "\n",
    "    @property\n",
    "    def doc_chain(self) -> LLMChain:\n",
    "        return load_qa_chain(llm=self.doc_llm, chain_type=\"stuff\")\n",
    "\n",
    "    @property\n",
    "    def qa(self) -> ConversationalRetrievalChain:\n",
    "        return ConversationalRetrievalChain(\n",
    "            retriever=self.vector_store.as_retriever(),\n",
    "            question_generator=self.question_generator,\n",
    "            combine_docs_chain=self.doc_chain,\n",
    "            verbose=True,\n",
    "        )\n",
    "\n",
    "    def _create_llm(self, model, streaming=False, callbacks=None) -> LLM:\n",
    "        \"\"\"\n",
    "        Determine the language model to be used.\n",
    "        :param model: Language model name to be used.\n",
    "        :param private_model_args: Dictionary containing model_path, n_ctx and n_batch.\n",
    "        :param private: Boolean value to determine if private model is to be used.\n",
    "        :return: Language model instance\n",
    "        \"\"\"\n",
    "        return ChatOpenAI(\n",
    "            temperature=0,\n",
    "            model=model,\n",
    "            streaming=streaming,\n",
    "            callbacks=callbacks,\n",
    "        )\n",
    "\n",
    "    def _call_chain(self, chain, question, history):\n",
    "        \"\"\"\n",
    "        Call a chain with a given question and history.\n",
    "        :param chain: The chain eg QA (ConversationalRetrievalChain)\n",
    "        :param question: The user prompt\n",
    "        :param history: The chat history from DB\n",
    "        :return: The answer.\n",
    "        \"\"\"\n",
    "        return chain(\n",
    "            {\n",
    "                \"question\": question,\n",
    "                \"chat_history\": history,\n",
    "            }\n",
    "        )\n",
    "\n",
    "    def generate_answer(self, question: str) -> ChatHistory:\n",
    "        \"\"\"\n",
    "        Generate an answer to a given question by interacting with the language model.\n",
    "        :param question: The question\n",
    "        :return: The generated answer.\n",
    "        \"\"\"\n",
    "        transformed_history = []\n",
    "\n",
    "        # Get the history from the database\n",
    "        history = get_chat_history(self.chat_id)\n",
    "\n",
    "        # Format the chat history into a list of tuples (human, ai)\n",
    "        transformed_history = format_chat_history(history)\n",
    "\n",
    "        # Generate the model response using the QA chain\n",
    "        model_response = self._call_chain(self.qa, question, transformed_history)\n",
    "\n",
    "        answer = model_response[\"answer\"]\n",
    "\n",
    "        # Update chat history\n",
    "        chat_answer = update_chat_history(\n",
    "            chat_id=self.chat_id,\n",
    "            user_message=question,\n",
    "            assistant=answer,\n",
    "        )\n",
    "\n",
    "        return chat_answer\n",
    "\n",
    "    async def _acall_chain(self, chain, question, history):\n",
    "        \"\"\"\n",
    "        Call a chain with a given question and history.\n",
    "        :param chain: The chain eg QA (ConversationalRetrievalChain)\n",
    "        :param question: The user prompt\n",
    "        :param history: The chat history from DB\n",
    "        :return: The answer.\n",
    "        \"\"\"\n",
    "        return chain.acall(\n",
    "            {\n",
    "                \"question\": question,\n",
    "                \"chat_history\": history,\n",
    "            }\n",
    "        )\n",
    "\n",
    "    async def generate_stream(self, question: str) -> AsyncIterable:\n",
    "        \"\"\"\n",
    "        Generate a streaming answer to a given question by interacting with the language model.\n",
    "        :param question: The question\n",
    "        :return: An async iterable which generates the answer.\n",
    "        \"\"\"\n",
    "\n",
    "        history = get_chat_history(self.chat_id)\n",
    "        callback = self.callbacks[0]\n",
    "\n",
    "        transformed_history = []\n",
    "\n",
    "        # Format the chat history into a list of tuples (human, ai)\n",
    "        transformed_history = format_chat_history(history)\n",
    "\n",
    "        # Initialize a list to hold the tokens\n",
    "        response_tokens = []\n",
    "\n",
    "        # Wrap an awaitable with a event to signal when it's done or an exception is raised.\n",
    "        async def wrap_done(fn: Awaitable, event: asyncio.Event):\n",
    "            try:\n",
    "                await fn\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Caught exception: {e}\")\n",
    "            finally:\n",
    "                event.set()\n",
    "\n",
    "        task = asyncio.create_task(\n",
    "            wrap_done(\n",
    "                self.qa._acall_chain(self.qa, question, transformed_history),\n",
    "                callback.done,\n",
    "            )\n",
    "        )\n",
    "\n",
    "        streamed_chat_history = update_chat_history(\n",
    "            chat_id=self.chat_id,\n",
    "            user_message=question,\n",
    "            assistant=\"\",\n",
    "        )\n",
    "\n",
    "        # Use the aiter method of the callback to stream the response with server-sent-events\n",
    "        async for token in callback.aiter():\n",
    "            logger.info(\"Token: %s\", token)\n",
    "\n",
    "            # Add the token to the response_tokens list\n",
    "            response_tokens.append(token)\n",
    "            streamed_chat_history.assistant = token\n",
    "\n",
    "            yield f\"data: {json.dumps(streamed_chat_history.to_dict())}\"\n",
    "\n",
    "        await task\n",
    "\n",
    "        # Join the tokens to create the assistant's response\n",
    "        assistant = \"\".join(response_tokens)\n",
    "\n",
    "        update_message_by_id(\n",
    "            message_id=streamed_chat_history.message_id,\n",
    "            user_message=question,\n",
    "            assistant=assistant,\n",
    "        )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Model function call "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "from typing import Any, Dict\n",
    "\n",
    "\n",
    "class FunctionCall:\n",
    "    def __init__(\n",
    "        self,\n",
    "        name: Optional[str] = None,\n",
    "        arguments: Optional[Dict[str, Any]] = None,\n",
    "    ):\n",
    "        self.name = name\n",
    "        self.arguments = arguments"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Model answer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "from .FunctionCall import FunctionCall\n",
    "\n",
    "\n",
    "class OpenAiAnswer:\n",
    "    def __init__(\n",
    "        self,\n",
    "        content: Optional[str] = None,\n",
    "        function_call: FunctionCall = None,\n",
    "    ):\n",
    "        self.content = content\n",
    "        self.function_call = function_call"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Condense prompt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts.prompt import PromptTemplate\n",
    "\n",
    "_template = \"\"\"Given the following conversation and a follow up question, rephrase the follow up question to be a standalone question, in its original language. include it in the standalone question.\n",
    "\n",
    "Chat History:\n",
    "{chat_history}\n",
    "Follow Up Input: {question}\n",
    "Standalone question:\"\"\"\n",
    "CONDENSE_QUESTION_PROMPT = PromptTemplate.from_template(_template)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Language prompt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts.prompt import PromptTemplate\n",
    "\n",
    "prompt_template = \"\"\"Your name is Quivr. You are a second brain. A person will ask you a question and you will provide a helpful answer. Write the answer in the same language as the question. If you don't know the answer, just say that you don't know. Don't try to make up an answer. Use the following context to answer the question:\n",
    "\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "Helpful Answer:\"\"\"\n",
    "QA_PROMPT = PromptTemplate(\n",
    "    template=prompt_template, input_variables=[\"context\", \"question\"]\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Summarization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import guidance\n",
    "import openai\n",
    "from logger import get_logger\n",
    "\n",
    "logger = get_logger(__name__)\n",
    "\n",
    "openai_api_key = os.environ.get(\"OPENAI_API_KEY\")\n",
    "openai.api_key = openai_api_key\n",
    "summary_llm = guidance.llms.OpenAI(\"gpt-3.5-turbo-0613\", caching=False)\n",
    "\n",
    "\n",
    "def llm_summerize(document):\n",
    "    summary = guidance(\n",
    "        \"\"\"\n",
    "{{#system~}}\n",
    "You are a world best summarizer. \\n\n",
    "Condense the text, capturing essential points and core ideas. Include relevant \\\n",
    "examples, omit excess details, and ensure the summary's length matches the \\\n",
    "original's complexity.\n",
    "{{/system~}}\n",
    "{{#user~}}\n",
    "Summarize the following text:\n",
    "---\n",
    "{{document}}\n",
    "{{/user~}}\n",
    "\n",
    "{{#assistant~}}\n",
    "{{gen 'summarization' temperature=0.2 max_tokens=100}}\n",
    "{{/assistant~}}\n",
    "\"\"\",\n",
    "        llm=summary_llm,\n",
    "    )\n",
    "\n",
    "    summary = summary(document=document)\n",
    "    logger.info(\"Summarization: %s\", summary)\n",
    "    return summary[\"summarization\"]\n",
    "\n",
    "\n",
    "def llm_evaluate_summaries(question, summaries, model):\n",
    "    if not model.startswith(\"gpt\"):\n",
    "        logger.info(f\"Model {model} not supported. Using gpt-3.5-turbo instead.\")\n",
    "        model = \"gpt-3.5-turbo-0613\"\n",
    "    logger.info(f\"Evaluating summaries with {model}\")\n",
    "    evaluation_llm = guidance.llms.OpenAI(model, caching=False)\n",
    "    evaluation = guidance(\n",
    "        \"\"\"\n",
    "{{#system~}}\n",
    "You are a world best evaluator. You evaluate the relevance of summaries based \\\n",
    "on user input question. Return evaluation in following csv format, csv headers \\\n",
    "are [summary_id,document_id,evaluation,reason].\n",
    "Evaluator Task\n",
    "- Evaluation should be a score number between 0 and 5.\n",
    "- Reason should be a short sentence within 20 words explain why the evaluation.\n",
    "---\n",
    "Example\n",
    "summary_id,document_id,evaluation,reason\n",
    "1,4,3,\"not mentioned about topic A\"\n",
    "2,2,4,\"It is not relevant to the question\"\n",
    "{{/system~}}\n",
    "{{#user~}}\n",
    "Based on the question, do Evaluator Task for each summary.\n",
    "---\n",
    "Question: {{question}}\n",
    "{{#each summaries}}\n",
    "Summary\n",
    "    summary_id: {{this.id}}\n",
    "    document_id: {{this.document_id}}\n",
    "    evaluation: \"\"\n",
    "    reason: \"\"\n",
    "    Summary Content: {{this.content}}\n",
    "    File Name: {{this.metadata.file_name}}\n",
    "{{/each}}\n",
    "{{/user~}}\n",
    "{{#assistant~}}\n",
    "{{gen 'evaluation' temperature=0.2 stop='<|im_end|>'}}\n",
    "{{/assistant~}}\n",
    "\"\"\",\n",
    "        llm=evaluation_llm,\n",
    "    )\n",
    "    result = evaluation(question=question, summaries=summaries)\n",
    "    evaluations = {}\n",
    "    for evaluation in result[\"evaluation\"].split(\"\\n\"):\n",
    "        if evaluation == \"\" or not evaluation[0].isdigit():\n",
    "            continue\n",
    "        logger.info(\"Evaluation Row: %s\", evaluation)\n",
    "        summary_id, document_id, score, *reason = evaluation.split(\",\")\n",
    "        if not score.isdigit():\n",
    "            continue\n",
    "        score = int(score)\n",
    "        if score < 3 or score > 5:\n",
    "            continue\n",
    "        evaluations[summary_id] = {\n",
    "            \"evaluation\": score,\n",
    "            \"reason\": \",\".join(reason),\n",
    "            \"summary_id\": summary_id,\n",
    "            \"document_id\": document_id,\n",
    "        }\n",
    "    return [\n",
    "        e\n",
    "        for e in sorted(\n",
    "            evaluations.values(), key=lambda x: x[\"evaluation\"], reverse=True\n",
    "        )\n",
    "    ]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Function call configuration "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Dict, List, Optional\n",
    "\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from llm.models.FunctionCall import FunctionCall\n",
    "from llm.models.OpenAiAnswer import OpenAiAnswer\n",
    "from logger import get_logger\n",
    "from models.chat import ChatHistory\n",
    "from repository.chat.get_chat_history import get_chat_history\n",
    "from repository.chat.update_chat_history import update_chat_history\n",
    "from supabase import Client, create_client\n",
    "from vectorstore.supabase import CustomSupabaseVectorStore\n",
    "\n",
    "from .base import BaseBrainPicking\n",
    "\n",
    "logger = get_logger(__name__)\n",
    "\n",
    "\n",
    "def format_answer(model_response: Dict[str, Any]) -> OpenAiAnswer:\n",
    "    answer = model_response[\"choices\"][0][\"message\"]\n",
    "    content = answer[\"content\"]\n",
    "    function_call = None\n",
    "\n",
    "    if answer.get(\"function_call\", None) is not None:\n",
    "        function_call = FunctionCall(\n",
    "            answer[\"function_call\"][\"name\"],\n",
    "            answer[\"function_call\"][\"arguments\"],\n",
    "        )\n",
    "\n",
    "    return OpenAiAnswer(content=content, function_call=function_call)\n",
    "\n",
    "\n",
    "class OpenAIFunctionsBrainPicking(BaseBrainPicking):\n",
    "    \"\"\"\n",
    "    Class for the OpenAI Brain Picking functionality using OpenAI Functions.\n",
    "    It allows to initialize a Chat model, generate questions and retrieve answers using ConversationalRetrievalChain.\n",
    "    \"\"\"\n",
    "\n",
    "    # Default class attributes\n",
    "    model: str = \"gpt-3.5-turbo-0613\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        model: str,\n",
    "        chat_id: str,\n",
    "        temperature: float,\n",
    "        max_tokens: int,\n",
    "        brain_id: str,\n",
    "        user_openai_api_key: str,\n",
    "        # TODO: add streaming\n",
    "    ) -> \"OpenAIFunctionsBrainPicking\":\n",
    "        super().__init__(\n",
    "            model=model,\n",
    "            chat_id=chat_id,\n",
    "            max_tokens=max_tokens,\n",
    "            user_openai_api_key=user_openai_api_key,\n",
    "            temperature=temperature,\n",
    "            brain_id=str(brain_id),\n",
    "            streaming=False,\n",
    "        )\n",
    "\n",
    "    @property\n",
    "    def openai_client(self) -> ChatOpenAI:\n",
    "        return ChatOpenAI(openai_api_key=self.openai_api_key)\n",
    "\n",
    "    @property\n",
    "    def embeddings(self) -> OpenAIEmbeddings:\n",
    "        return OpenAIEmbeddings(openai_api_key=self.openai_api_key)\n",
    "\n",
    "    @property\n",
    "    def supabase_client(self) -> Client:\n",
    "        return create_client(\n",
    "            self.brain_settings.supabase_url, self.brain_settings.supabase_service_key\n",
    "        )\n",
    "\n",
    "    @property\n",
    "    def vector_store(self) -> CustomSupabaseVectorStore:\n",
    "        return CustomSupabaseVectorStore(\n",
    "            self.supabase_client,\n",
    "            self.embeddings,\n",
    "            table_name=\"vectors\",\n",
    "            brain_id=self.brain_id,\n",
    "        )\n",
    "\n",
    "    def _get_model_response(\n",
    "        self,\n",
    "        messages: List[Dict[str, str]],\n",
    "        functions: Optional[List[Dict[str, Any]]] = None,\n",
    "    ) -> Any:\n",
    "        \"\"\"\n",
    "        Retrieve a model response given messages and functions\n",
    "        \"\"\"\n",
    "        logger.info(\"Getting model response\")\n",
    "        kwargs = {\n",
    "            \"messages\": messages,\n",
    "            \"model\": self.model,\n",
    "            \"temperature\": self.temperature,\n",
    "            \"max_tokens\": self.max_tokens,\n",
    "        }\n",
    "\n",
    "        if functions:\n",
    "            logger.info(\"Adding functions to model response\")\n",
    "            kwargs[\"functions\"] = functions\n",
    "\n",
    "        return self.openai_client.completion_with_retry(**kwargs)\n",
    "\n",
    "    def _get_chat_history(self) -> List[Dict[str, str]]:\n",
    "        \"\"\"\n",
    "        Retrieves the chat history in a formatted list\n",
    "        \"\"\"\n",
    "        logger.info(\"Getting chat history\")\n",
    "        history = get_chat_history(self.chat_id)\n",
    "        return [\n",
    "            item\n",
    "            for chat in history\n",
    "            for item in [\n",
    "                {\"role\": \"user\", \"content\": chat.user_message},\n",
    "                {\"role\": \"assistant\", \"content\": chat.assistant},\n",
    "            ]\n",
    "        ]\n",
    "\n",
    "    def _get_context(self, question: str) -> str:\n",
    "        \"\"\"\n",
    "        Retrieve documents related to the question\n",
    "        \"\"\"\n",
    "        logger.info(\"Getting context\")\n",
    "\n",
    "        return self.vector_store.similarity_search(query=question)\n",
    "\n",
    "    def _construct_prompt(\n",
    "        self, question: str, useContext: bool = False, useHistory: bool = False\n",
    "    ) -> List[Dict[str, str]]:\n",
    "        \"\"\"\n",
    "        Constructs a prompt given a question, and optionally include context and history\n",
    "        \"\"\"\n",
    "        logger.info(\"Constructing prompt\")\n",
    "        system_messages = [\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"\"\"Your name is Quivr. You are an assistant that has access to a person's documents and that can answer questions about them.\n",
    "                A person will ask you a question and you will provide a helpful answer. \n",
    "                Write the answer in the same language as the question. \n",
    "                You have access to functions to help you answer the question.\n",
    "                If you don't know the answer, just say that you don't know but be helpful and explain why you can't answer\"\"\",\n",
    "            }\n",
    "        ]\n",
    "\n",
    "        if useHistory:\n",
    "            logger.info(\"Adding chat history to prompt\")\n",
    "            history = self._get_chat_history()\n",
    "            system_messages.append(\n",
    "                {\"role\": \"system\", \"content\": \"Previous messages are already in chat.\"}\n",
    "            )\n",
    "            system_messages.extend(history)\n",
    "\n",
    "        if useContext:\n",
    "            logger.info(\"Adding chat context to prompt\")\n",
    "            chat_context = self._get_context(question)\n",
    "            context_message = f\"Here are the documents you have access to: {chat_context if chat_context else 'No document found'}\"\n",
    "            system_messages.append({\"role\": \"user\", \"content\": context_message})\n",
    "\n",
    "        system_messages.append({\"role\": \"user\", \"content\": question})\n",
    "\n",
    "        return system_messages\n",
    "\n",
    "    def generate_answer(self, question: str) -> ChatHistory:\n",
    "        \"\"\"\n",
    "        Main function to get an answer for the given question\n",
    "        \"\"\"\n",
    "        logger.info(\"Getting answer\")\n",
    "        functions = [\n",
    "            {\n",
    "                \"name\": \"get_history_and_context\",\n",
    "                \"description\": \"Get the chat history between you and the user and also get the relevant documents to answer the question. Always use that unless a very simple question is asked that a 5 years old could answer.\",\n",
    "                \"parameters\": {\"type\": \"object\", \"properties\": {}},\n",
    "            },\n",
    "        ]\n",
    "\n",
    "        # First, try to get an answer using just the question\n",
    "        response = self._get_model_response(\n",
    "            messages=self._construct_prompt(question), functions=functions\n",
    "        )\n",
    "        formatted_response = format_answer(response)\n",
    "\n",
    "        # If the model calls for history, try again with history included\n",
    "        if (\n",
    "            formatted_response.function_call\n",
    "            and formatted_response.function_call.name == \"get_history\"\n",
    "        ):\n",
    "            logger.info(\"Model called for history\")\n",
    "            response = self._get_model_response(\n",
    "                messages=self._construct_prompt(question, useHistory=True),\n",
    "                functions=[],\n",
    "            )\n",
    "\n",
    "            formatted_response = format_answer(response)\n",
    "\n",
    "        if (\n",
    "            formatted_response.function_call\n",
    "            and formatted_response.function_call.name == \"get_history_and_context\"\n",
    "        ):\n",
    "            logger.info(\"Model called for history and context\")\n",
    "            response = self._get_model_response(\n",
    "                messages=self._construct_prompt(\n",
    "                    question, useContext=True, useHistory=True\n",
    "                ),\n",
    "                functions=[],\n",
    "            )\n",
    "            formatted_response = format_answer(response)\n",
    "\n",
    "        # Update chat history\n",
    "        chat_history = update_chat_history(\n",
    "            chat_id=self.chat_id,\n",
    "            user_message=question,\n",
    "            assistant=formatted_response.content or \"\",\n",
    "        )\n",
    "\n",
    "        return chat_history"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
